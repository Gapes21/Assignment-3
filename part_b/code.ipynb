{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7915632,"sourceType":"datasetVersion","datasetId":4651040}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchsummary\n# imports\n\nimport torch\nfrom torch import nn\nfrom torchvision.datasets import FashionMNIST\nfrom torchvision import transforms\nfrom torch.utils.data import random_split, DataLoader\nfrom torchsummary import summary\nfrom torch.nn import CrossEntropyLoss\nfrom torch.optim import Adam\nfrom tqdm.auto import tqdm\nimport time\nfrom sklearn.metrics import confusion_matrix, accuracy_score","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:09:13.236132Z","iopub.execute_input":"2024-03-22T17:09:13.236879Z","iopub.status.idle":"2024-03-22T17:09:25.013098Z","shell.execute_reply.started":"2024-03-22T17:09:13.236848Z","shell.execute_reply":"2024-03-22T17:09:25.011935Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchsummary in /opt/conda/lib/python3.10/site-packages (1.5.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#!pip install torchsummary\n# imports\n\nimport torch\nfrom torch import nn\nfrom torchvision.datasets import FashionMNIST\nfrom torchvision import transforms\nfrom torch.utils.data import random_split, DataLoader\nfrom torchsummary import summary\nfrom torch.nn import CrossEntropyLoss\nfrom torch.optim import Adam\nfrom tqdm.auto import tqdm\nimport time\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom torchvision.datasets import ImageFolder\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:09:32.255940Z","iopub.execute_input":"2024-03-22T17:09:32.256635Z","iopub.status.idle":"2024-03-22T17:09:32.264235Z","shell.execute_reply.started":"2024-03-22T17:09:32.256579Z","shell.execute_reply":"2024-03-22T17:09:32.263312Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":" tfs = transforms.Compose([\n     transforms.Resize((48, 64)),\n     transforms.ToTensor()\n ])\n #temp_dataset = ImageFolder(\"/kaggle/input/bikehorse/Assignment2_BikeHorses\", transform=tfs)\n train_dataset = ImageFolder(\"/kaggle/input/bikeshorses/Assignment2_BikeHorses/Train\", transform=tfs)\n val_dataset = ImageFolder(\"/kaggle/input/bikeshorses/Assignment2_BikeHorses/Test\", transform=tfs)\n len(train_dataset), len(val_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T15:44:45.374964Z","iopub.execute_input":"2024-03-22T15:44:45.375506Z","iopub.status.idle":"2024-03-22T15:44:45.457985Z","shell.execute_reply.started":"2024-03-22T15:44:45.375476Z","shell.execute_reply":"2024-03-22T15:44:45.457150Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(165, 14)"},"metadata":{}}]},{"cell_type":"code","source":"# setup device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice = torch.device(device)\ndevice","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for reproducibility\ntorch.manual_seed(42)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:09:42.402263Z","iopub.execute_input":"2024-03-22T17:09:42.402669Z","iopub.status.idle":"2024-03-22T17:09:42.410204Z","shell.execute_reply.started":"2024-03-22T17:09:42.402637Z","shell.execute_reply":"2024-03-22T17:09:42.409035Z"},"trusted":true},"execution_count":68,"outputs":[{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x78ffb498f3d0>"},"metadata":{}}]},{"cell_type":"code","source":"# fashionmnist\ntrain_dataset = FashionMNIST(\"./data\", download=True, transform=transforms.ToTensor())\nval_dataset = FashionMNIST(\"./data\", download=True, train=False, transform=transforms.ToTensor())\nlen(train_dataset), len(val_dataset) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Food101\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets, transforms\n\n# Define transformations (adjust for image size)\ntransform = transforms.Compose([\n    transforms.Resize(256),  # Resize images (adjust as needed)\n    transforms.CenterCrop(224),  # Crop central area\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # ImageNet normalization\n])\n\n# Download and load Food101 dataset\ntrain_dataset = datasets.Food101(\"./data\", download=True, transform=transform)\nval_dataset = datasets.Food101(\"./data\", train=False, download=True, transform=transform)\n\n# Print dataset lengths\nprint(f\"Number of training samples: {len(train_dataset)}\")\nprint(f\"Number of validation samples: {len(val_dataset)}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#SVHN\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets, transforms\n\n# Define transformations (may require adjustments for SVHN)\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize pixel values\n])\n\n# Download and load SVHN dataset\ntrain_dataset = datasets.SVHN(\"./data\", split='train', download=True, transform=transform)\nval_dataset = datasets.SVHN(\"./data\", split='test', download=True, transform=transform)\n\n# Print dataset lengths\nprint(f\"Number of training samples: {len(train_dataset)}\")\nprint(f\"Number of validation samples: {len(val_dataset)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:09:55.161235Z","iopub.execute_input":"2024-03-22T17:09:55.161625Z","iopub.status.idle":"2024-03-22T17:09:58.232334Z","shell.execute_reply.started":"2024-03-22T17:09:55.161594Z","shell.execute_reply":"2024-03-22T17:09:58.231366Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"Using downloaded and verified file: ./data/train_32x32.mat\nUsing downloaded and verified file: ./data/test_32x32.mat\nNumber of training samples: 73257\nNumber of validation samples: 26032\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset[0][0].shape","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:04:28.764329Z","iopub.execute_input":"2024-03-22T17:04:28.764693Z","iopub.status.idle":"2024-03-22T17:04:28.775868Z","shell.execute_reply.started":"2024-03-22T17:04:28.764662Z","shell.execute_reply":"2024-03-22T17:04:28.774972Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"torch.Size([3, 32, 32])"},"metadata":{}}]},{"cell_type":"code","source":"# create dataloaders\nbatch_size = 128\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:10:04.141321Z","iopub.execute_input":"2024-03-22T17:10:04.141679Z","iopub.status.idle":"2024-03-22T17:10:04.151806Z","shell.execute_reply.started":"2024-03-22T17:10:04.141650Z","shell.execute_reply":"2024-03-22T17:10:04.150810Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"class Classifier(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        \n        self.block_1 = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=\"same\"),\n            nn.BatchNorm2d(32),\n           nn.Sigmoid(),\n           nn.MaxPool2d(2)\n        )\n        #elf.block_1 = nn.Sequential(\n    #n.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=\"same\"),  # Change in_channels to 3\n    #n.BatchNorm2d(32),\n    #n.ReLU(),\n    #n.MaxPool2d(2)\n#\n\n        self.block_2 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=\"same\"),\n            nn.BatchNorm2d(64),\n            nn.Sigmoid(),\n            nn.MaxPool2d(2)\n        )\n        self.block_3 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=\"same\"),\n            nn.BatchNorm2d(128),\n            nn.Sigmoid(),\n            nn.MaxPool2d(2)\n        )\n\n        self.block_4 = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(in_features=2048, out_features=64),\n            nn.Sigmoid(),\n        )\n        self.last_layer = nn.Linear(in_features=64, out_features=10)\n        \n    def forward(self, x):\n        features = self.block_4(\n            self.block_3(\n                self.block_2(\n                    self.block_1(x))))\n        activations = self.last_layer(features)\n        return (activations, features)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:36:44.133985Z","iopub.execute_input":"2024-03-22T17:36:44.134872Z","iopub.status.idle":"2024-03-22T17:36:44.145046Z","shell.execute_reply.started":"2024-03-22T17:36:44.134838Z","shell.execute_reply":"2024-03-22T17:36:44.144244Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"model = Classifier().to(device)\nsummary(model, (3, 32 , 32))\n#summary(model,(3,28,28))","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:36:49.308534Z","iopub.execute_input":"2024-03-22T17:36:49.308911Z","iopub.status.idle":"2024-03-22T17:36:49.336206Z","shell.execute_reply.started":"2024-03-22T17:36:49.308882Z","shell.execute_reply":"2024-03-22T17:36:49.335348Z"},"trusted":true},"execution_count":93,"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 32, 32, 32]             896\n       BatchNorm2d-2           [-1, 32, 32, 32]              64\n           Sigmoid-3           [-1, 32, 32, 32]               0\n         MaxPool2d-4           [-1, 32, 16, 16]               0\n            Conv2d-5           [-1, 64, 16, 16]          18,496\n       BatchNorm2d-6           [-1, 64, 16, 16]             128\n           Sigmoid-7           [-1, 64, 16, 16]               0\n         MaxPool2d-8             [-1, 64, 8, 8]               0\n            Conv2d-9            [-1, 128, 8, 8]          73,856\n      BatchNorm2d-10            [-1, 128, 8, 8]             256\n          Sigmoid-11            [-1, 128, 8, 8]               0\n        MaxPool2d-12            [-1, 128, 4, 4]               0\n          Flatten-13                 [-1, 2048]               0\n           Linear-14                   [-1, 64]         131,136\n          Sigmoid-15                   [-1, 64]               0\n           Linear-16                   [-1, 10]             650\n================================================================\nTotal params: 225,482\nTrainable params: 225,482\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.01\nForward/backward pass size (MB): 1.44\nParams size (MB): 0.86\nEstimated Total Size (MB): 2.31\n----------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"def train_step(model: torch.nn.Module,\n               data_loader: torch.utils.data.DataLoader,\n               loss_fn: torch.nn.Module,\n               optimizer: torch.optim.Optimizer,\n               device: torch.device\n              ):\n    \n    start_time = time.time()\n\n    train_loss = 0    \n    model.to(device)\n    model.train()\n    \n    for (X, y) in data_loader:\n        # send data to GPU\n        X, y = X.to(device), y.to(device)\n        # X, y = X.to(device), y.type(torch.LongTensor).to(device)\n        \n        # 1. forward pass\n        y_pred, _ = model(X)\n\n        # 2. calculate loss\n        loss = loss_fn(y_pred, y)\n        train_loss += loss\n        \n        # 3. optimizer zero grad\n        optimizer.zero_grad()\n        \n        # 4. loss backward\n        loss.backward()\n        \n        # 5. optimizer step\n        optimizer.step()\n    \n    train_loss /= len(data_loader)\n\n    end_time = time.time()\n\n    return {\"avg_batch_loss\": train_loss, \"time\": (end_time - start_time)* 10**3}\n\ndef valid_step(model: torch.nn.Module,\n               data_loader: torch.utils.data.DataLoader,\n               device: torch.device\n              ):\n    \n    # send the model to device\n    model.to(device)\n\n    # send the model in eval mode\n    model.eval()\n\n    # for confusion matrix and accuracy\n    y_true = torch.Tensor([]).to(device)\n    y_pred = torch.Tensor([]).to(device)\n\n    with torch.inference_mode(): \n        for X, y in data_loader:\n            # Send data to GPU\n            X, y = X.to(device), y.to(device)\n            \n            # 1. Forward pass\n            test_pred, _ = model(X)\n            \n            y_true = torch.cat((y_true, y), dim=0)\n            y_pred = torch.cat((y_pred, test_pred.argmax(axis=1)), dim=0)\n        \n        # send back to cpu\n        y_true = y_true.cpu()\n        y_pred = y_pred.cpu()\n\n        return {\"accuracy\": accuracy_score(y_true, y_pred), \"confusion_matrix\": confusion_matrix(y_true, y_pred, normalize=\"true\")}\n","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:36:53.484548Z","iopub.execute_input":"2024-03-22T17:36:53.484911Z","iopub.status.idle":"2024-03-22T17:36:53.497477Z","shell.execute_reply.started":"2024-03-22T17:36:53.484882Z","shell.execute_reply":"2024-03-22T17:36:53.496464Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"# create loss_fn\n\nloss_fn = CrossEntropyLoss()\n\n# create optimizer\n\nlr = 3.2 * (10**-4)\noptimizer = Adam(model.parameters(), lr=lr)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:37:03.110303Z","iopub.execute_input":"2024-03-22T17:37:03.110669Z","iopub.status.idle":"2024-03-22T17:37:03.116360Z","shell.execute_reply.started":"2024-03-22T17:37:03.110639Z","shell.execute_reply":"2024-03-22T17:37:03.115331Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"epochs = 32\n\nfor epoch in tqdm(range(epochs)):\n        tres = train_step(model, train_dataloader, loss_fn, optimizer, device)\n        print(f\"epoch: {epoch}\")\n        print(f\"avg_batch_loss: {tres['avg_batch_loss']}\")\n        print(f\"time: {tres['time']}\")   \n        print(\"\")","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:37:06.622823Z","iopub.execute_input":"2024-03-22T17:37:06.623514Z","iopub.status.idle":"2024-03-22T17:47:33.446105Z","shell.execute_reply.started":"2024-03-22T17:37:06.623481Z","shell.execute_reply":"2024-03-22T17:47:33.445225Z"},"trusted":true},"execution_count":97,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"912b177927d449379d2cc16ca19bd5d5"}},"metadata":{}},{"name":"stdout","text":"epoch: 0\navg_batch_loss: 1.6132841110229492\ntime: 19293.6110496521\n\nepoch: 1\navg_batch_loss: 0.7847603559494019\ntime: 19639.333963394165\n\nepoch: 2\navg_batch_loss: 0.565780520439148\ntime: 19340.041160583496\n\nepoch: 3\navg_batch_loss: 0.47616514563560486\ntime: 19744.540691375732\n\nepoch: 4\navg_batch_loss: 0.42506811022758484\ntime: 19550.634384155273\n\nepoch: 5\navg_batch_loss: 0.3904785215854645\ntime: 19656.18848800659\n\nepoch: 6\navg_batch_loss: 0.3644222915172577\ntime: 19625.080347061157\n\nepoch: 7\navg_batch_loss: 0.3431127071380615\ntime: 19754.719972610474\n\nepoch: 8\navg_batch_loss: 0.32560887932777405\ntime: 19266.436338424683\n\nepoch: 9\navg_batch_loss: 0.3104700446128845\ntime: 19858.958959579468\n\nepoch: 10\navg_batch_loss: 0.2967403829097748\ntime: 19662.429809570312\n\nepoch: 11\navg_batch_loss: 0.28390929102897644\ntime: 19377.522945404053\n\nepoch: 12\navg_batch_loss: 0.2723960280418396\ntime: 19892.36092567444\n\nepoch: 13\navg_batch_loss: 0.2615979313850403\ntime: 19414.748668670654\n\nepoch: 14\navg_batch_loss: 0.25142261385917664\ntime: 19468.10746192932\n\nepoch: 15\navg_batch_loss: 0.24145974218845367\ntime: 19754.125356674194\n\nepoch: 16\navg_batch_loss: 0.23264148831367493\ntime: 19589.169025421143\n\nepoch: 17\navg_batch_loss: 0.2236890345811844\ntime: 19416.19062423706\n\nepoch: 18\navg_batch_loss: 0.21542106568813324\ntime: 19641.849040985107\n\nepoch: 19\navg_batch_loss: 0.2070549875497818\ntime: 19498.541116714478\n\nepoch: 20\navg_batch_loss: 0.19894540309906006\ntime: 19655.5757522583\n\nepoch: 21\navg_batch_loss: 0.19147075712680817\ntime: 19518.664836883545\n\nepoch: 22\navg_batch_loss: 0.18383191525936127\ntime: 19547.60694503784\n\nepoch: 23\navg_batch_loss: 0.17695939540863037\ntime: 19421.16069793701\n\nepoch: 24\navg_batch_loss: 0.16969716548919678\ntime: 19218.918800354004\n\nepoch: 25\navg_batch_loss: 0.16307614743709564\ntime: 19391.956090927124\n\nepoch: 26\navg_batch_loss: 0.15659117698669434\ntime: 19470.33429145813\n\nepoch: 27\navg_batch_loss: 0.1502416878938675\ntime: 19296.146154403687\n\nepoch: 28\navg_batch_loss: 0.14444689452648163\ntime: 19942.7330493927\n\nepoch: 29\navg_batch_loss: 0.13880817592144012\ntime: 19953.584671020508\n\nepoch: 30\navg_batch_loss: 0.1329185515642166\ntime: 19832.833290100098\n\nepoch: 31\navg_batch_loss: 0.12740638852119446\ntime: 19698.278427124023\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# validation results\nvres = valid_step(model, val_dataloader, device)\nprint(f\"accuracy: {vres['accuracy']}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:47:44.551237Z","iopub.execute_input":"2024-03-22T17:47:44.551584Z","iopub.status.idle":"2024-03-22T17:47:50.665985Z","shell.execute_reply.started":"2024-03-22T17:47:44.551558Z","shell.execute_reply":"2024-03-22T17:47:50.664964Z"},"trusted":true},"execution_count":98,"outputs":[{"name":"stdout","text":"accuracy: 0.8915949600491703\nconfusion_matrix: \n[[8.54357798e-01 2.58027523e-02 1.72018349e-03 8.02752294e-03\n  2.86697248e-03 5.73394495e-03 4.07110092e-02 4.58715596e-03\n  2.23623853e-02 3.38302752e-02]\n [5.09903903e-03 9.55285350e-01 1.96116886e-03 4.51068837e-03\n  1.21592469e-02 4.70680526e-03 2.54951951e-03 8.04079231e-03\n  2.35340263e-03 3.33398706e-03]\n [1.68715353e-03 1.75946011e-02 8.87924801e-01 2.43432152e-02\n  1.15690528e-02 1.25331405e-02 2.89226320e-03 1.27741624e-02\n  1.06049651e-02 1.80766450e-02]\n [1.04094379e-03 2.84524636e-02 9.02151284e-03 8.18528799e-01\n  5.20471895e-03 6.21096461e-02 6.24566273e-03 4.16377516e-03\n  2.98403886e-02 3.53920888e-02]\n [2.77447483e-03 4.24098296e-02 5.54894966e-03 1.03051922e-02\n  9.06064209e-01 8.32342449e-03 8.71977804e-03 3.56718193e-03\n  3.96353547e-03 8.32342449e-03]\n [1.67785235e-03 8.38926174e-03 3.35570470e-03 2.05536913e-02\n  8.38926174e-03 9.27852349e-01 1.59395973e-02 8.38926174e-04\n  6.71140940e-03 6.29194631e-03]\n [8.59888720e-03 1.51745068e-02 2.02326758e-03 2.32675771e-02\n  1.21396055e-02 5.10875063e-02 8.46737481e-01 2.02326758e-03\n  2.98431968e-02 9.10470410e-03]\n [9.90589401e-04 6.88459633e-02 1.04011887e-02 1.04011887e-02\n  1.98117880e-03 1.23823675e-02 2.97176820e-03 8.82119861e-01\n  3.46706290e-03 6.43883110e-03]\n [4.21686747e-03 9.03614458e-03 2.40963855e-03 2.77108434e-02\n  9.03614458e-03 3.19277108e-02 3.01204819e-02 1.20481928e-03\n  8.58433735e-01 2.59036145e-02]\n [1.19122257e-02 1.69278997e-02 1.88087774e-02 9.40438871e-03\n  8.77742947e-03 1.94357367e-02 3.76175549e-03 5.01567398e-03\n  1.06583072e-02 8.95297806e-01]]\n","output_type":"stream"}]},{"cell_type":"code","source":"# get features\ndef get_features(model, data_loader):\n    # send the model to device\n    model.to(device)\n\n    # send the model in eval mode\n    model.eval()\n\n    # for confusion matrix and accuracy\n    all_y = torch.Tensor([]).to(device)\n    all_x = torch.Tensor([]).to(device)\n\n    with torch.inference_mode(): \n        for X, y in data_loader:\n            # Send data to GPU\n            X, y = X.to(device), y.to(device)\n            \n            # 1. Forward pass\n            _, features = model(X)\n            \n            all_y = torch.cat((all_y, y), dim=0)\n            all_x = torch.cat((all_x, features), dim=0)\n        \n        # send back to cpu\n        return (all_x.cpu(), all_y.cpu())   ","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:47:56.904468Z","iopub.execute_input":"2024-03-22T17:47:56.905308Z","iopub.status.idle":"2024-03-22T17:47:56.912286Z","shell.execute_reply.started":"2024-03-22T17:47:56.905272Z","shell.execute_reply":"2024-03-22T17:47:56.911332Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"train_x, train_y = get_features(model, train_dataloader)\ntrain_x.shape, train_y.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:48:01.467299Z","iopub.execute_input":"2024-03-22T17:48:01.467664Z","iopub.status.idle":"2024-03-22T17:48:18.510385Z","shell.execute_reply.started":"2024-03-22T17:48:01.467634Z","shell.execute_reply":"2024-03-22T17:48:18.509433Z"},"trusted":true},"execution_count":100,"outputs":[{"execution_count":100,"output_type":"execute_result","data":{"text/plain":"(torch.Size([73257, 64]), torch.Size([73257]))"},"metadata":{}}]},{"cell_type":"code","source":"val_x, val_y = get_features(model, val_dataloader)\nval_x.shape, val_y.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:48:33.738707Z","iopub.execute_input":"2024-03-22T17:48:33.739069Z","iopub.status.idle":"2024-03-22T17:48:39.632741Z","shell.execute_reply.started":"2024-03-22T17:48:33.739039Z","shell.execute_reply":"2024-03-22T17:48:39.631863Z"},"trusted":true},"execution_count":101,"outputs":[{"execution_count":101,"output_type":"execute_result","data":{"text/plain":"(torch.Size([26032, 64]), torch.Size([26032]))"},"metadata":{}}]},{"cell_type":"code","source":"# try out sklearn models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score \n\nfmodel = LogisticRegression(max_iter=1024)\nfmodel.fit(train_x, train_y)\nprint(\"accuracy:\", accuracy_score(val_y, fmodel.predict(val_x)))","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:48:43.170417Z","iopub.execute_input":"2024-03-22T17:48:43.170777Z","iopub.status.idle":"2024-03-22T17:49:16.615600Z","shell.execute_reply.started":"2024-03-22T17:48:43.170745Z","shell.execute_reply":"2024-03-22T17:49:16.614155Z"},"trusted":true},"execution_count":102,"outputs":[{"name":"stdout","text":"accuracy: 0.9018515673017824\n","output_type":"stream"}]},{"cell_type":"code","source":"# random forest\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score \n\nfmodel = RandomForestClassifier()\nfmodel.fit(train_x, train_y)\nprint(\"accuracy:\", accuracy_score(val_y, fmodel.predict(val_x)))","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:49:27.421145Z","iopub.execute_input":"2024-03-22T17:49:27.421526Z","iopub.status.idle":"2024-03-22T17:51:16.850641Z","shell.execute_reply.started":"2024-03-22T17:49:27.421494Z","shell.execute_reply":"2024-03-22T17:51:16.849594Z"},"trusted":true},"execution_count":103,"outputs":[{"name":"stdout","text":"accuracy: 0.9031960663798402\n","output_type":"stream"}]},{"cell_type":"code","source":"# gaussian NB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score \n\nfmodel = GaussianNB()\nfmodel.fit(train_x, train_y)\nprint(\"accuracy:\", accuracy_score(val_y, fmodel.predict(val_x)))","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:51:24.669474Z","iopub.execute_input":"2024-03-22T17:51:24.670129Z","iopub.status.idle":"2024-03-22T17:51:24.778631Z","shell.execute_reply.started":"2024-03-22T17:51:24.670095Z","shell.execute_reply":"2024-03-22T17:51:24.777562Z"},"trusted":true},"execution_count":104,"outputs":[{"name":"stdout","text":"accuracy: 0.8942839582052858\n","output_type":"stream"}]},{"cell_type":"code","source":"# multinomial NB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score \n\nfmodel = MultinomialNB()\nfmodel.fit(train_x, train_y)\nprint(\"accuracy:\", accuracy_score(val_y, fmodel.predict(val_x)))","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:51:43.974985Z","iopub.execute_input":"2024-03-22T17:51:43.975821Z","iopub.status.idle":"2024-03-22T17:51:44.025372Z","shell.execute_reply.started":"2024-03-22T17:51:43.975787Z","shell.execute_reply":"2024-03-22T17:51:44.021997Z"},"trusted":true},"execution_count":105,"outputs":[{"name":"stdout","text":"accuracy: 0.8948985863552551\n","output_type":"stream"}]},{"cell_type":"code","source":"#SVM\nfrom sklearn.svm import SVC\n\nfmodel = SVC(kernel='linear')\nfmodel.fit(train_x, train_y)\nprint(\"accuracy:\", accuracy_score(val_y, fmodel.predict(val_x)))\n","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:25:11.501666Z","iopub.execute_input":"2024-03-22T17:25:11.502542Z","iopub.status.idle":"2024-03-22T17:25:24.497456Z","shell.execute_reply.started":"2024-03-22T17:25:11.502502Z","shell.execute_reply":"2024-03-22T17:25:24.496234Z"},"trusted":true},"execution_count":86,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[86], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m fmodel \u001b[38;5;241m=\u001b[39m SVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m fmodel\u001b[38;5;241m.\u001b[39mfit(train_x, train_y)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_score(val_y, \u001b[43mfmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_x\u001b[49m\u001b[43m)\u001b[49m))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:820\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    818\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39masarray(y, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:435\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    433\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_for_predict(X)\n\u001b[1;32m    434\u001b[0m predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_predict \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dense_predict\n\u001b[0;32m--> 435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:454\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    447\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX.shape[1] = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m should be equal to \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe number of samples at training time\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m             \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    450\u001b[0m         )\n\u001b[1;32m    452\u001b[0m svm_type \u001b[38;5;241m=\u001b[39m LIBSVM_IMPL\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlibsvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupport_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupport_vectors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_n_support\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dual_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_intercept_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_probA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_probB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43msvm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msvm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"#Gradient Boosting\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfmodel = GradientBoostingClassifier(n_estimators=50, learning_rate=0.1, max_depth=1)\n\nfmodel.fit(train_x, train_y)\n\nprint(\"accuracy:\", accuracy_score(val_y, fmodel.predict(val_x)))\n","metadata":{"execution":{"iopub.status.busy":"2024-03-22T15:50:22.094030Z","iopub.execute_input":"2024-03-22T15:50:22.094883Z","iopub.status.idle":"2024-03-22T15:50:22.154684Z","shell.execute_reply.started":"2024-03-22T15:50:22.094848Z","shell.execute_reply":"2024-03-22T15:50:22.153714Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"accuracy: 1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"#SVM\nfrom sklearn.svm import LinearSVC\n\nfmodel = LinearSVC(max_iter=5000)\nfmodel.fit(train_x, train_y)\nprint(\"accuracy:\", accuracy_score(val_y, fmodel.predict(val_x)))","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:51:54.838134Z","iopub.execute_input":"2024-03-22T17:51:54.838729Z","iopub.status.idle":"2024-03-22T17:52:12.926143Z","shell.execute_reply.started":"2024-03-22T17:51:54.838698Z","shell.execute_reply":"2024-03-22T17:52:12.922632Z"},"trusted":true},"execution_count":106,"outputs":[{"name":"stdout","text":"accuracy: 0.9016210817455439\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Create a KNN classifier with 5 neighbors (you can adjust this later)\nknn_model = KNeighborsClassifier(n_neighbors=10)\n\n# Train the model on the training data\nknn_model.fit(train_x, train_y)\n\n# Make predictions on the validation data\npredictions = knn_model.predict(val_x)\n\n# Evaluate model accuracy\naccuracy = accuracy_score(val_y, predictions)\nprint(\"accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:52:22.707999Z","iopub.execute_input":"2024-03-22T17:52:22.708403Z","iopub.status.idle":"2024-03-22T17:52:29.385783Z","shell.execute_reply.started":"2024-03-22T17:52:22.708373Z","shell.execute_reply":"2024-03-22T17:52:29.384845Z"},"trusted":true},"execution_count":107,"outputs":[{"name":"stdout","text":"accuracy: 0.9058850645359557\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Create an Extra Trees Classifier instance\netc_model = ExtraTreesClassifier()\n\n# Fit the model to your training data\netc_model.fit(train_x, train_y)\n\n# Make predictions on the validation set\npredictions = etc_model.predict(val_x)\n\n# Calculate and print the accuracy\naccuracy = accuracy_score(val_y, predictions)\nprint(\"accuracy:\", accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:52:39.340295Z","iopub.execute_input":"2024-03-22T17:52:39.340626Z","iopub.status.idle":"2024-03-22T17:52:53.168301Z","shell.execute_reply.started":"2024-03-22T17:52:39.340602Z","shell.execute_reply":"2024-03-22T17:52:53.167340Z"},"trusted":true},"execution_count":108,"outputs":[{"name":"stdout","text":"accuracy: 0.9029271665642287\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.naive_bayes import BernoulliNB\nfrom sklearn.metrics import accuracy_score\n\n# Create a BernoulliNB classifier instance\nbnb_model = BernoulliNB()\n\n# Fit the model to your training data\nbnb_model.fit(train_x, train_y)\n\n# Make predictions on the validation set\npredictions = bnb_model.predict(val_x)\n\n# Calculate and print the accuracy\naccuracy = accuracy_score(val_y, predictions)\nprint(\"accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:53:14.853893Z","iopub.execute_input":"2024-03-22T17:53:14.854274Z","iopub.status.idle":"2024-03-22T17:53:14.917846Z","shell.execute_reply.started":"2024-03-22T17:53:14.854242Z","shell.execute_reply":"2024-03-22T17:53:14.916483Z"},"trusted":true},"execution_count":110,"outputs":[{"name":"stdout","text":"accuracy: 0.1958743085433313\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.svm import LinearSVC\n\n# Create a LinearSVC classifier instance with L1 penalty\nsvc_model = LinearSVC(penalty='l1', dual=False)  # Set penalty to 'l1' for L1 regularization\n\n# Fit the model to your training data\nsvc_model.fit(train_x, train_y)\n\n# Make predictions on the validation set\npredictions = svc_model.predict(val_x)\n\n# Calculate and print the accuracy\naccuracy = accuracy_score(val_y, predictions)\nprint(\"accuracy:\", accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-22T17:53:22.490578Z","iopub.execute_input":"2024-03-22T17:53:22.490955Z","iopub.status.idle":"2024-03-22T17:58:25.613904Z","shell.execute_reply.started":"2024-03-22T17:53:22.490924Z","shell.execute_reply":"2024-03-22T17:58:25.609785Z"},"trusted":true},"execution_count":111,"outputs":[{"name":"stdout","text":"accuracy: 0.9015442532267978\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n","output_type":"stream"}]}]}